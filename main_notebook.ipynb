{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593eb79b",
   "metadata": {},
   "source": [
    "# P2: Dramatic Data!\n",
    "\n",
    "\n",
    "## Table Of Content\n",
    "\n",
    "1. Introduction\n",
    "2. Preliminaries\n",
    "3. Software Setup\n",
    "5. Grading Rubric\n",
    "6. Submission guidelines\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In this project, you will design and implement a deep neural network capable of segmenting PeAR racing windows.\n",
    "A sample output is shown below,\n",
    "\n",
    "![Segmentation Image](./assets/segsample.jpg)\n",
    "\n",
    "This project has three parts,\n",
    "- Part 1 - Synthetic dataset generation\n",
    "- Part 2 - Training and Validating the CNN\n",
    "- Part 3 - Instance Segmentation (identify different instances of the window)\n",
    "\n",
    "### Part 1 - Dataset generation\n",
    "No dataset has been provided for this project. You are required to generate a synthetic dataset for training your network. The network must generalize to the real-world video feed given in test_video.mp4. For more details, please refer to section 2.1.\n",
    "\n",
    "### Part 2 - Training and validating the CNN\n",
    "You will be designing a custom CNN for image segmentation, and then run inference on frames from the provided video file (test_video.mp4). Your task is to display the inferred segmentation output frames side by side with the original input frames and render the combined frames as a new video.\n",
    "\n",
    "Please note that test_video.mp4 contains multiple windows. Your network is expected to segment all the windows present in each input frame.\n",
    "\n",
    "### Part 3 - Instance segmentation\n",
    "\n",
    "This section is more open-ended. You will need to perform instance segmentation, which can be achieved either by applying classical techniques on the segmentation output obtained in Part 2, or by using deep learning methods. No starter code or specific instructions are provided for this section, allowing you the freedom to choose your approach.\n",
    "\n",
    "\n",
    "**Please review the submission guidelines and grading rubric before starting your work.**\n",
    "\n",
    "## 2. Preliminaries\n",
    "\n",
    "### 2.1. Dataset generation and sim2real:\n",
    "\n",
    "We highly recommend using Blender for generating your dataset. The window image is located at ./assets/window.png.\n",
    "\n",
    "You will need to generate the following:\n",
    "\n",
    "1. **Realistic RGB Images:** These images should contain the provided window in various settings.\n",
    "2. **Segmentation Masks:** Create binary images that indicate the presence of the window in the RGB images. Cycles rendering engine in Blender can output ID Masks for you. [reference](https://www.youtube.com/watch?v=o2JKviMX9rE)\n",
    "\n",
    "![Sample Data](./assets/sampledata.png)\n",
    "\n",
    "To ensure your network generalizes well from simulation to the real world, make sure to incorporate the following variations in your dataset:\n",
    "1. **Camera location and orientation:** Vary the position and angle of the camera relative to the window.\n",
    "1. **Background:** Add some background image. A few images are provided in ./part1/environment for your reference.\n",
    "1. **Lighting:** Simulate various lighting conditions.\n",
    "1. **Occlusion:** Add objects that block the window.\n",
    "1. **Multiple Windows:** Show scenes with more than one window.\n",
    "1. **Noise:** Introduce gaussian noise to the image.\n",
    "1. **Blur:** Add blur to the image to simulate out of focus conditions.\n",
    "1. **Color Jitter:** Adjust color properties to simulate diverse scenarios.\n",
    "\n",
    "You will receive full credit for `Part 1` if your dataset images are properly augmented with the factors mentioned above.\n",
    "\n",
    "You are free to use any gui/commandline operation. The following tools may be helpful for automating the data generation,\n",
    "1. [Blender script](https://docs.blender.org/api/current/info_quickstart.html) - Dataset generation (randomizing the placement of camera/windows, automated rendering, etc).\n",
    "2. [BlenderSynth (beta)](https://github.com/OllieBoyne/BlenderSynth) - Dataset generation.\n",
    "3. [TorchVision](https://pytorch.org/vision/stable/generated/torchvision.transforms.GaussianBlur.html) - Data augmentation (noise, blur,...) on-the-fly during training.\n",
    "\n",
    "**Recommended Rendering Settings:**\n",
    "\n",
    "In case you don’t know where these settings are, don’t worry! You are free to use any settings you like. \n",
    "\n",
    "1. Rendering Engine - Cycles (with GPU compute enabled)\n",
    "2. Max Samples - 4\n",
    "3. Noise Threshold - 0.1\n",
    "4. Denoise - Optix \n",
    "5. Final Render/ Persistent Data - Enabled (this gives 10x speed up but Blender may crash). Try rendering few 100 images at a time.\n",
    "6. Render resolution - 640x360\n",
    "\n",
    "**Sample Dataset Images**\n",
    "\n",
    "![Sample Dataset](./assets/sampleDataset.png)\n",
    "\n",
    "You’ll need a total of approximately 50,000 augmented images for effective training. We generated around 5,000 images using Blender, and then applied augmentations such as noise, blur, and color jitter during the training process using torchvision.\n",
    "\n",
    "### 2.2 Segmentation network in PyTorch:\n",
    "\n",
    "Architectures similar to U-Net are recommended as a good starting point.\n",
    "To keep the design simple and to speed up inference, you are free to rescale the image to smaller square format (such as 256x256). Please make sure you scale the segmented output back to the original dimension.\n",
    "\n",
    "Here is a helpful article on [image segmentation with Pytorch](https://towardsdatascience.com/efficient-image-segmentation-using-pytorch-part-1-89e8297a0923)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f305e",
   "metadata": {},
   "source": [
    "## 3. Software Setup\n",
    "\n",
    "### Part 1\n",
    "No sample code for data generation is provided. However, a few sample background images can be found in the part1/environment folder, and the window image is available in the assets/ folder. Please add any code you write in the part1 folder.\n",
    "\n",
    "### Part 2\n",
    "A sample network training pipeline (and wandb logging code) is provided for reference. You are free to edit it as you like.\n",
    "You are expected to fix any issues you face while running that pipeline. Please use [wandb](https://docs.wandb.ai/tutorials/experiments) or tensorboard to visualize the training process. \n",
    "<!-- ![Sample Wandb Webpage](./assets/wandb.png) -->\n",
    "\n",
    "<img src=\"./assets/wandb.png\" alt=\"Sample Wandb Image\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54272187",
   "metadata": {},
   "source": [
    "## 4. Grading Rubric\n",
    "\n",
    "- part 1: 40\n",
    "- part 2: 40\n",
    "- part 3: 20\n",
    "\n",
    "- For RBE474X: part1 + part2 = 100% of the grade (80/80).\n",
    "- For RBE595-A01-SP: You are expected to implement part1-part3 for getting full credits (100/100).\n",
    "\n",
    "## 5. Submission Guidelines\n",
    "\n",
    "### Report\n",
    "\n",
    "Please include the following in your report,\n",
    "\n",
    "1. If you are using Blender, screenshots of your Blender GUI Window.\n",
    "2. Sample images and labels (segmentation mask) from the dataset.\n",
    "3. Network architecture diagram.\n",
    "4. Explain the loss function you used for training.\n",
    "5. Explain the failure cases, you may have encountered.\n",
    "6. Tabulate the hyperparameters you used for training. Learning rate, optimizer, etc.\n",
    "7. Include the inference results for a few sample images from your validation set and the provided video.\n",
    "8. Plot the training and validation loss curves per epoch. Explain any observation.\n",
    "\n",
    "### Video\n",
    "\n",
    "Part 2: Save the video as part2.mp4. Please use H.264 encoding for the video.\n",
    "\n",
    "Part 3: Save the video as part3.mp4. Please use H.264 encoding for the video.\n",
    "\n",
    "### Rules\n",
    "\n",
    "1. You can choose any network architecture, but you must design it yourself. Prebuilt networks or pre-existing implementations (e.g., those found in libraries like PyTorch's torchvision) are not allowed.\n",
    "2. You are allowed to use basic PyTorch layers, such as convolutional layers, transposed convolutions, pooling layers, activation functions, dropout, and batch normalization.\n",
    "3. Do not upload any logs or model_checkpoint(pth) or data or wandb folder. Doing so will result in zero credits.\n",
    "4. Do not upload your dataset to Canvas! Doing so will result in zero credits.\n",
    "5. Report must be in Latex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6175d",
   "metadata": {},
   "source": [
    "### Folder Structure\n",
    "Your submission on ELMS/Canvas must be a ``zip`` file, following the naming convention ``GroupGROUPNUM_p2.zip``. If your group number is ``4``, then the submission file should be named ``Group4_p2.zip``. The `GROUPNUM` can be found on Canvas. The file **must have the following directory structure**. Do not change the files to run the code. You can have any helper functions in sub-folders as you wish, be sure to index them using relative paths and if you have command line arguments for your codes, make sure to have default values too. Please provide detailed instructions on how to run your code in ``README.md`` file. \n",
    "\n",
    "<p style=\"background-color:#ddd; padding:5px\">\n",
    "<b>NOTE:</b> \n",
    "Furthermore, the size of your submission file should <b>NOT</b> exceed more than <b>100MB</b>.\n",
    "</p>\n",
    "\n",
    "The file tree of your submission <b>SHOULD</b> resemble this:\n",
    "\n",
    "```\n",
    "GroupGROUPNUM_p2.zip\n",
    "├── assets\n",
    "├── part1\n",
    "    ├── code files (do not submit environment folder)\n",
    "├── part2\n",
    "    ├── network.py\n",
    "    ├── train.py\n",
    "    ├── turing.sh\n",
    "    ├── loadParam.py\n",
    "    ├── dataloader.py\n",
    "    ├── utils.py\n",
    "    ├── Any other code files\n",
    "├── part3\n",
    "    ├── code files\n",
    "├── Report.pdf \n",
    "├── main_notebook.ipynb\n",
    "├── part2.mp4\n",
    "├── part3.mp4\n",
    "└── README.md\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
